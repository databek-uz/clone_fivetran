# ğŸš€ PipeZone

**Open-source Fivetran-like data pipeline framework â€” fully YAML-defined**

PipeZone is a modern data integration platform that allows you to build, schedule, and monitor data pipelines using simple YAML configurations. Extract data from various sources, transform it with Spark, and load it into your data lake with zero coding required.

## âœ¨ Features

- ğŸ”Œ **Multiple Source Connectors**: PostgreSQL, MySQL, SQL Server, Oracle, MongoDB, and more
- ğŸ“¦ **Data Lake Architecture**: MinIO-based object storage with Raw â†’ Bronze â†’ Silver â†’ Gold layers
- ğŸ”„ **Smart Incremental Sync**: Automatic state management for incremental data loads
- âš¡ **Spark Processing**: Distributed data processing with PySpark
- ğŸ“Š **Airflow Orchestration**: Automatic DAG generation from YAML flows
- ğŸ” **Secrets Management**: HashiCorp Vault integration for secure credential storage
- ğŸ““ **Jupyter Integration**: Interactive development environment with all database drivers
- ğŸ³ **Docker-based**: Easy deployment with Docker Compose
- ğŸ“ **YAML-Defined**: No coding required - define everything in YAML

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PipeZone Platform                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Sources    â”‚â”€â”€â”€â–¶â”‚  Spark Jobs  â”‚â”€â”€â”€â–¶â”‚  Data Lake   â”‚  â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ â€¢ PostgreSQL â”‚    â”‚ â€¢ Transform  â”‚    â”‚ â€¢ Raw        â”‚  â”‚
â”‚  â”‚ â€¢ MySQL      â”‚    â”‚ â€¢ Validate   â”‚    â”‚ â€¢ Bronze     â”‚  â”‚
â”‚  â”‚ â€¢ MongoDB    â”‚    â”‚ â€¢ Enrich     â”‚    â”‚ â€¢ Silver     â”‚  â”‚
â”‚  â”‚ â€¢ APIs       â”‚    â”‚              â”‚    â”‚ â€¢ Gold       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                              â”‚                               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                    â”‚  Airflow Scheduler â”‚                     â”‚
â”‚                    â”‚  (Auto-generated)  â”‚                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Infrastructure: MySQL â€¢ MinIO â€¢ Vault â€¢ Spark â€¢ Jupyter    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
pipezone/
â”œâ”€â”€ core/                       # Core processing logic
â”‚   â”œâ”€â”€ notebooks/             # Jupyter notebooks for development
â”‚   â”œâ”€â”€ jobs/                  # Production Spark jobs
â”‚   â”œâ”€â”€ transformations/       # Data transformation logic
â”‚   â”œâ”€â”€ utils/                 # Utility modules
â”‚   â”‚   â”œâ”€â”€ connection_manager.py  # Connection management
â”‚   â”‚   â””â”€â”€ flow_executor.py       # Flow execution engine
â”‚   â””â”€â”€ drivers/               # Database drivers
â”‚
â”œâ”€â”€ schedule/                   # Orchestration
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ dags/              # Airflow DAGs (auto-generated)
â”‚       â”œâ”€â”€ plugins/           # Custom plugins
â”‚       â””â”€â”€ logs/              # Airflow logs
â”‚
â”œâ”€â”€ metadata/                   # YAML configurations
â”‚   â”œâ”€â”€ connections/           # Connection definitions
â”‚   â”‚   â”œâ”€â”€ postgres_source.yml
â”‚   â”‚   â”œâ”€â”€ mysql_source.yml
â”‚   â”‚   â”œâ”€â”€ minio_raw.yml
â”‚   â”‚   â””â”€â”€ minio_bronze.yml
â”‚   â”œâ”€â”€ flows/                 # Data flow definitions
â”‚   â”‚   â”œâ”€â”€ postgres_users_to_raw.yml
â”‚   â”‚   â”œâ”€â”€ mysql_orders_to_raw.yml
â”‚   â”‚   â””â”€â”€ raw_to_bronze_users.yml
â”‚   â””â”€â”€ schemas/               # Schema definitions
â”‚
â”œâ”€â”€ setup/                      # Setup & deployment
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ docker-compose.infra.yml     # MySQL, MinIO, Vault
â”‚   â”‚   â”œâ”€â”€ docker-compose.airflow.yml   # Airflow services
â”‚   â”‚   â”œâ”€â”€ docker-compose.notebooks.yml # Jupyter + Spark
â”‚   â”‚   â”œâ”€â”€ Dockerfile.jupyter           # Custom Jupyter image
â”‚   â”‚   â””â”€â”€ init-scripts/                # Initialization scripts
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ start-all.sh       # Start all services
â”‚       â””â”€â”€ stop-all.sh        # Stop all services
â”‚
â”œâ”€â”€ data/                       # Docker volumes (gitignored)
â”‚   â”œâ”€â”€ mysql/                 # MySQL data
â”‚   â”œâ”€â”€ minio/                 # MinIO data
â”‚   â”œâ”€â”€ vault/                 # Vault data
â”‚   â””â”€â”€ notebooks/             # Jupyter data
â”‚
â””â”€â”€ .env                        # Environment configuration
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- At least 8GB RAM available
- Ports available: 3306, 8080, 8888, 9000, 9001, 8200, 7077, 8081, 8082

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/pipezone.git
cd pipezone
```

### 2. Configure Environment

The `.env` file is already configured with default values. Review and update as needed:

```bash
# Review the .env file
cat .env

# Update credentials if needed
nano .env
```

### 3. Start All Services

```bash
bash setup/scripts/start-all.sh
```

This will start:
- âœ… MySQL (metadata storage)
- âœ… MinIO (object storage with raw/bronze/silver/gold buckets)
- âœ… Vault (secrets management)
- âœ… Airflow (scheduler + webserver)
- âœ… Spark (master + worker)
- âœ… Jupyter (with PySpark + all database drivers)

### 4. Access the Platform

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow UI | http://localhost:8080 | admin / admin |
| MinIO Console | http://localhost:9001 | pipezone_admin / pipezone_minio_2024 |
| Jupyter Lab | http://localhost:8888 | token: pipezone2024 |
| Vault UI | http://localhost:8200 | token: pipezone-dev-token-2024 |
| Spark Master | http://localhost:8081 | - |
| Spark Worker | http://localhost:8082 | - |

## ğŸ“ Creating Your First Pipeline

### Step 1: Define a Connection

Create a connection YAML in `metadata/connections/`:

```yaml
# metadata/connections/postgres_production.yml
name: postgres_production
type: postgresql
description: Production PostgreSQL database

connection:
  host: ${POSTGRES_HOST}
  port: 5432
  database: ${POSTGRES_DATABASE}
  username: ${POSTGRES_USER}
  password: ${POSTGRES_PASSWORD}

vault:
  enabled: true
  path: secret/data/postgres/production
  keys:
    username: username
    password: password

metadata:
  owner: data_engineering
  tags:
    - production
    - postgresql
```

### Step 2: Define a Data Flow

Create a flow YAML in `metadata/flows/`:

```yaml
# metadata/flows/extract_customers.yml
name: extract_customers
description: Extract customers from PostgreSQL to MinIO raw layer

source:
  connection: postgres_production
  type: table
  table:
    schema: public
    name: customers
  incremental:
    enabled: true
    column: updated_at
    strategy: max_value

target:
  connection: minio_raw
  type: object_storage
  path: postgres/customers/
  format: parquet
  partition_by:
    - country
    - year
    - month
  write_mode: append
  compression: snappy

data_quality:
  enabled: true
  checks:
    - name: unique_customer_id
      type: unique
      columns: [customer_id]

    - name: email_valid
      type: regex
      column: email
      pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

schedule:
  enabled: true
  cron: "0 */6 * * *"  # Every 6 hours
  timezone: Asia/Tashkent

metadata:
  owner: data_engineering
  sla: 6h
  priority: high
  tags:
    - customers
    - incremental
```

### Step 3: Run the Pipeline

**Option A: Via Airflow (Automatic)**

1. Go to Airflow UI: http://localhost:8080
2. Find your DAG: `pipezone_extract_customers`
3. Enable it and trigger manually, or wait for scheduled run

**Option B: Via Jupyter Notebook**

```python
from flow_executor import FlowExecutor

# Initialize executor
executor = FlowExecutor()

# Execute flow
result = executor.execute_flow('extract_customers')

print(f"Status: {result['status']}")
print(f"Records: {result['records_written']}")
print(f"Duration: {result['duration_seconds']}s")
```

**Option C: Via Python Script**

```bash
docker exec pipezone_jupyter python -c "
from sys import path
path.insert(0, '/home/pipezone/utils')
from flow_executor import FlowExecutor
result = FlowExecutor().execute_flow('extract_customers')
print(result)
"
```

## ğŸ”„ Data Flow Patterns

### Pattern 1: Incremental Sync

Automatically track the last loaded value and only extract new/updated records:

```yaml
incremental:
  enabled: true
  column: updated_at
  strategy: max_value
```

### Pattern 2: Full Refresh

Complete reload of the source data:

```yaml
incremental:
  enabled: false

target:
  write_mode: overwrite
```

### Pattern 3: Raw â†’ Bronze Transformation

Clean and standardize raw data:

```yaml
# metadata/flows/raw_to_bronze_customers.yml
source:
  connection: minio_raw
  path: postgres/customers/
  format: parquet

target:
  connection: minio_bronze
  path: customers/
  format: parquet

transformation:
  enabled: true
  steps:
    - name: remove_duplicates
      type: deduplicate
      columns: [customer_id]

    - name: clean_email
      type: transform
      column: email
      expression: "lower(trim(email))"

    - name: add_metadata
      type: add_columns
      columns:
        processed_at: "current_timestamp()"
        layer: "'bronze'"
```

## ğŸ—„ï¸ Supported Data Sources

### Databases
- âœ… PostgreSQL
- âœ… MySQL / MariaDB
- âœ… SQL Server
- âœ… Oracle
- âœ… MongoDB
- âœ… Cassandra
- âœ… Redis
- âœ… Elasticsearch
- âœ… ClickHouse
- âœ… Snowflake

### File Systems
- âœ… MinIO / S3
- âœ… Local filesystem
- âœ… HDFS

All JDBC drivers and Python connectors are pre-installed in the Jupyter container!

## ğŸ“Š Monitoring & Logs

### Execution Logs

All pipeline executions are logged to MySQL:

```sql
SELECT * FROM pipezone_metadata.pipeline_execution_logs
ORDER BY start_time DESC
LIMIT 10;
```

### Incremental State

Check the current incremental state:

```sql
SELECT * FROM pipezone_metadata.incremental_state;
```

### Airflow Logs

```bash
docker-compose -f setup/docker/docker-compose.airflow.yml logs -f airflow-scheduler
```

### Spark Logs

```bash
docker-compose -f setup/docker/docker-compose.notebooks.yml logs -f spark-master
```

## ğŸ› ï¸ Development Workflow

### 1. Develop in Jupyter

Access Jupyter at http://localhost:8888 (token: `pipezone2024`)

```python
# Load your data
from connection_manager import ConnectionManager

cm = ConnectionManager()
config = cm.load_connection_config('postgres_production')

# Test connection
cm.test_connection('postgres_production')

# Use with Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.read \
    .format("jdbc") \
    .option("url", cm.get_connection_string('postgres_production')) \
    .option("dbtable", "public.customers") \
    .load()

df.show()
```

### 2. Test Flow Locally

```python
from flow_executor import FlowExecutor

executor = FlowExecutor()
result = executor.execute_flow('your_flow_name')
```

### 3. Deploy to Airflow

Simply enable the schedule in your flow YAML:

```yaml
schedule:
  enabled: true
  cron: "0 */6 * * *"
```

Airflow will automatically pick up the flow and create a DAG!

## ğŸ” Security Best Practices

### Use Vault for Secrets

1. Store secrets in Vault:

```bash
docker exec -it pipezone_vault sh

vault kv put secret/postgres/production \
  username=myuser \
  password=mypassword
```

2. Reference in connection YAML:

```yaml
vault:
  enabled: true
  path: secret/data/postgres/production
  keys:
    username: username
    password: password
```

### Environment Variables

Never commit `.env` to git! Use `.env.example` as a template.

## ğŸ§ª Testing

### Test Connections

```python
from connection_manager import ConnectionManager

cm = ConnectionManager()

# List all connections
connections = cm.list_connections()

# Test each connection
for conn in connections:
    status = "âœ“" if cm.test_connection(conn) else "âœ—"
    print(f"{status} {conn}")
```

### Validate Flow Config

```python
from flow_executor import FlowExecutor

executor = FlowExecutor()
config = executor.load_flow_config('your_flow_name')
print(config)
```

## ğŸ“¦ Data Quality

Built-in data quality checks:

```yaml
data_quality:
  enabled: true
  checks:
    - name: row_count
      type: not_null
      threshold: 0

    - name: unique_id
      type: unique
      columns: [id]

    - name: email_format
      type: regex
      column: email
      pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

    - name: amount_range
      type: range
      column: amount
      min: 0
      max: 1000000
```

Results are stored in `data_quality_checks` table.

## ğŸ³ Docker Commands

```bash
# Start all services
bash setup/scripts/start-all.sh

# Stop all services
bash setup/scripts/stop-all.sh

# View logs
docker-compose -f setup/docker/docker-compose.infra.yml logs -f

# Restart a specific service
docker-compose -f setup/docker/docker-compose.airflow.yml restart airflow-scheduler

# Access container shell
docker exec -it pipezone_jupyter bash
docker exec -it pipezone_mysql mysql -u root -p
```

## ğŸ”§ Troubleshooting

### Services won't start

```bash
# Check Docker resources
docker system df

# Clean up
docker system prune -a

# Restart Docker daemon
sudo systemctl restart docker
```

### Airflow DAGs not appearing

```bash
# Check logs
docker-compose -f setup/docker/docker-compose.airflow.yml logs -f airflow-scheduler

# Verify metadata path is mounted
docker exec pipezone_airflow_scheduler ls -la /opt/pipezone/metadata/flows
```

### Spark connection issues

```bash
# Check Spark master is running
curl http://localhost:8081

# Verify network
docker network inspect pipezone_network
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

Inspired by:
- Fivetran
- Airbyte
- Apache Airflow
- Delta Lake

---

**Built with â¤ï¸ for the data engineering community**

For questions and support, please open an issue on GitHub.
