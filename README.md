# üöÄ PipeZone

**Open-source Fivetran-like data pipeline framework ‚Äî fully YAML-defined**

PipeZone is a modern data integration platform that allows you to build, schedule, and monitor data pipelines using simple YAML configurations. Extract data from various sources, transform it with Spark, and load it into your data lake with zero coding required.

## ‚ú® Features

- üîå **Multiple Source Connectors**: PostgreSQL, MySQL, SQL Server, Oracle, MongoDB, and more
- üì¶ **Data Lake Architecture**: MinIO-based object storage with Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold layers
- üîÑ **Smart Incremental Sync**: Automatic state management for incremental data loads
- ‚ö° **Spark Processing**: Distributed data processing with PySpark
- üìä **Airflow Orchestration**: Automatic DAG generation from YAML flows
- üîê **Secrets Management**: HashiCorp Vault integration for secure credential storage
- üìì **Jupyter Integration**: Interactive development environment with all database drivers
- üê≥ **Docker-based**: Easy deployment with Docker Compose
- üìù **YAML-Defined**: No coding required - define everything in YAML

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     PipeZone Platform                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Sources    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Spark Jobs  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Data Lake   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ PostgreSQL ‚îÇ    ‚îÇ ‚Ä¢ Transform  ‚îÇ    ‚îÇ ‚Ä¢ Raw        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ MySQL      ‚îÇ    ‚îÇ ‚Ä¢ Validate   ‚îÇ    ‚îÇ ‚Ä¢ Bronze     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ MongoDB    ‚îÇ    ‚îÇ ‚Ä¢ Enrich     ‚îÇ    ‚îÇ ‚Ä¢ Silver     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ APIs       ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ ‚Ä¢ Gold       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                    ‚îÇ                    ‚îÇ          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                              ‚îÇ                               ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ                    ‚îÇ  Airflow Scheduler ‚îÇ                     ‚îÇ
‚îÇ                    ‚îÇ  (Auto-generated)  ‚îÇ                     ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Infrastructure: MySQL ‚Ä¢ MinIO ‚Ä¢ Vault ‚Ä¢ Spark ‚Ä¢ Jupyter    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
pipezone/
‚îú‚îÄ‚îÄ core/                       # Core processing logic
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/             # Jupyter notebooks for development
‚îÇ   ‚îú‚îÄ‚îÄ jobs/                  # Production Spark jobs
‚îÇ   ‚îú‚îÄ‚îÄ transformations/       # Data transformation logic
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utility modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection_manager.py  # Connection management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ flow_executor.py       # Flow execution engine
‚îÇ   ‚îî‚îÄ‚îÄ drivers/               # Database drivers
‚îÇ
‚îú‚îÄ‚îÄ schedule/                   # Orchestration
‚îÇ   ‚îî‚îÄ‚îÄ airflow/
‚îÇ       ‚îú‚îÄ‚îÄ dags/              # Airflow DAGs (auto-generated)
‚îÇ       ‚îú‚îÄ‚îÄ plugins/           # Custom plugins
‚îÇ       ‚îî‚îÄ‚îÄ logs/              # Airflow logs
‚îÇ
‚îú‚îÄ‚îÄ metadata/                   # YAML configurations
‚îÇ   ‚îú‚îÄ‚îÄ connections/           # Connection definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres_source.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mysql_source.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ minio_raw.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ minio_bronze.yml
‚îÇ   ‚îú‚îÄ‚îÄ flows/                 # Data flow definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres_users_to_raw.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mysql_orders_to_raw.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ raw_to_bronze_users.yml
‚îÇ   ‚îî‚îÄ‚îÄ schemas/               # Schema definitions
‚îÇ
‚îú‚îÄ‚îÄ setup/                      # Setup & deployment
‚îÇ   ‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.infra.yml     # MySQL, MinIO, Vault
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.airflow.yml   # Airflow services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.notebooks.yml # Jupyter + Spark
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.jupyter           # Custom Jupyter image
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ init-scripts/                # Initialization scripts
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ start-all.sh       # Start all services
‚îÇ       ‚îî‚îÄ‚îÄ stop-all.sh        # Stop all services
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Docker volumes (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ mysql/                 # MySQL data
‚îÇ   ‚îú‚îÄ‚îÄ minio/                 # MinIO data
‚îÇ   ‚îú‚îÄ‚îÄ vault/                 # Vault data
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/             # Jupyter data
‚îÇ
‚îî‚îÄ‚îÄ .env                        # Environment configuration
```

## üöÄ Quick Start

### Prerequisites

- **Linux/Mac**: Docker & Docker Compose
- **Windows**: Docker Desktop for Windows (see [WINDOWS_SETUP.md](WINDOWS_SETUP.md))
- At least 8GB RAM available for Docker
- Ports available: 3306, 8080, 8888, 9000, 9001, 8200, 7077, 8081, 8082

> üí° **Windows Users**: Please refer to [WINDOWS_SETUP.md](WINDOWS_SETUP.md) for Windows-specific setup instructions using PowerShell or CMD scripts.

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/pipezone.git
cd pipezone
```

### 2. Configure Environment

Copy the example environment file and configure your credentials:

```bash
# Copy the example file
cp .env.example .env

# Edit with your credentials
nano .env
```

**Important configurations to update:**

```bash
# MySQL passwords
MYSQL_ROOT_PASSWORD=your_secure_password_here
MYSQL_PASSWORD=your_secure_password_here

# MinIO credentials
MINIO_ROOT_USER=your_admin_username
MINIO_ROOT_PASSWORD=your_secure_password_here

# Airflow admin password
AIRFLOW_ADMIN_PASSWORD=your_admin_password
AIRFLOW_DATABASE_PASSWORD=your_db_password

# Jupyter token
JUPYTER_TOKEN=your_jupyter_token

# Vault token
VAULT_TOKEN=your_vault_token

# Add your external database connections
POSTGRES_SOURCE_HOST=your-postgres-host.com
POSTGRES_SOURCE_DATABASE=your_database
POSTGRES_SOURCE_USER=your_username
POSTGRES_SOURCE_PASSWORD=your_password
```

üí° **Tip**: The `.env.example` file contains examples for all supported data sources (PostgreSQL, MySQL, MongoDB, SQL Server, Oracle, Snowflake, etc.). Uncomment and configure the ones you need.

### 3. Start All Services

**Linux/Mac:**
```bash
bash setup/scripts/start-all.sh
```

**Windows PowerShell:**
```powershell
.\setup\scripts\start-all.ps1
```

**Windows CMD:**
```cmd
setup\scripts\start-all.bat
```

> üí° **Windows Users**: See [WINDOWS_SETUP.md](WINDOWS_SETUP.md) for detailed Windows instructions.

This will start:
- ‚úÖ MySQL (metadata storage)
- ‚úÖ MinIO (object storage with raw/bronze/silver/gold buckets)
- ‚úÖ Vault (secrets management)
- ‚úÖ Airflow (scheduler + webserver)
- ‚úÖ Spark (master + worker)
- ‚úÖ Jupyter (with PySpark + all database drivers)

### 4. Access the Platform

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow UI | http://localhost:8080 | admin / admin |
| MinIO Console | http://localhost:9001 | pipezone_admin / pipezone_minio_2024 |
| Jupyter Lab | http://localhost:8888 | token: pipezone2024 |
| Vault UI | http://localhost:8200 | token: pipezone-dev-token-2024 |
| Spark Master | http://localhost:8081 | - |
| Spark Worker | http://localhost:8082 | - |

## üìù Creating Your First Pipeline

### Step 1: Define a Connection

Create a connection YAML in `metadata/connections/`:

```yaml
# metadata/connections/postgres_production.yml
name: postgres_production
type: postgresql
description: Production PostgreSQL database

connection:
  host: ${POSTGRES_HOST}
  port: 5432
  database: ${POSTGRES_DATABASE}
  username: ${POSTGRES_USER}
  password: ${POSTGRES_PASSWORD}

vault:
  enabled: true
  path: secret/data/postgres/production
  keys:
    username: username
    password: password

metadata:
  owner: data_engineering
  tags:
    - production
    - postgresql
```

### Step 2: Define a Data Flow

Create a flow YAML in `metadata/flows/`:

```yaml
# metadata/flows/extract_customers.yml
name: extract_customers
description: Extract customers from PostgreSQL to MinIO raw layer

source:
  connection: postgres_production
  type: table
  table:
    schema: public
    name: customers
  incremental:
    enabled: true
    column: updated_at
    strategy: max_value

target:
  connection: minio_raw
  type: object_storage
  path: postgres/customers/
  format: parquet
  partition_by:
    - country
    - year
    - month
  write_mode: append
  compression: snappy

data_quality:
  enabled: true
  checks:
    - name: unique_customer_id
      type: unique
      columns: [customer_id]

    - name: email_valid
      type: regex
      column: email
      pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

schedule:
  enabled: true
  cron: "0 */6 * * *"  # Every 6 hours
  timezone: Asia/Tashkent

metadata:
  owner: data_engineering
  sla: 6h
  priority: high
  tags:
    - customers
    - incremental
```

### Step 3: Run the Pipeline

**Option A: Via Airflow (Automatic)**

1. Go to Airflow UI: http://localhost:8080
2. Find your DAG: `pipezone_extract_customers`
3. Enable it and trigger manually, or wait for scheduled run

**Option B: Via Jupyter Notebook**

```python
from flow_executor import FlowExecutor

# Initialize executor
executor = FlowExecutor()

# Execute flow
result = executor.execute_flow('extract_customers')

print(f"Status: {result['status']}")
print(f"Records: {result['records_written']}")
print(f"Duration: {result['duration_seconds']}s")
```

**Option C: Via Python Script**

```bash
docker exec pipezone_jupyter python -c "
from sys import path
path.insert(0, '/home/pipezone/utils')
from flow_executor import FlowExecutor
result = FlowExecutor().execute_flow('extract_customers')
print(result)
"
```

## üîÑ Data Flow Patterns

### Pattern 1: Incremental Sync

Automatically track the last loaded value and only extract new/updated records:

```yaml
incremental:
  enabled: true
  column: updated_at
  strategy: max_value
```

### Pattern 2: Full Refresh

Complete reload of the source data:

```yaml
incremental:
  enabled: false

target:
  write_mode: overwrite
```

### Pattern 3: Raw ‚Üí Bronze Transformation

Clean and standardize raw data:

```yaml
# metadata/flows/raw_to_bronze_customers.yml
source:
  connection: minio_raw
  path: postgres/customers/
  format: parquet

target:
  connection: minio_bronze
  path: customers/
  format: parquet

transformation:
  enabled: true
  steps:
    - name: remove_duplicates
      type: deduplicate
      columns: [customer_id]

    - name: clean_email
      type: transform
      column: email
      expression: "lower(trim(email))"

    - name: add_metadata
      type: add_columns
      columns:
        processed_at: "current_timestamp()"
        layer: "'bronze'"
```

## üóÑÔ∏è Supported Data Sources

### Databases
- ‚úÖ PostgreSQL
- ‚úÖ MySQL / MariaDB
- ‚úÖ SQL Server
- ‚úÖ Oracle
- ‚úÖ MongoDB
- ‚úÖ Cassandra
- ‚úÖ Redis
- ‚úÖ Elasticsearch
- ‚úÖ ClickHouse
- ‚úÖ Snowflake

### File Systems
- ‚úÖ MinIO / S3
- ‚úÖ Local filesystem
- ‚úÖ HDFS

All JDBC drivers and Python connectors are pre-installed in the Jupyter container!

## üìä Monitoring & Logs

### Execution Logs

All pipeline executions are logged to MySQL:

```sql
SELECT * FROM pipezone_metadata.pipeline_execution_logs
ORDER BY start_time DESC
LIMIT 10;
```

### Incremental State

Check the current incremental state:

```sql
SELECT * FROM pipezone_metadata.incremental_state;
```

### Airflow Logs

```bash
docker-compose -f setup/docker/docker-compose.airflow.yml logs -f airflow-scheduler
```

### Spark Logs

```bash
docker-compose -f setup/docker/docker-compose.notebooks.yml logs -f spark-master
```

## üõ†Ô∏è Development Workflow

### 1. Develop in Jupyter

Access Jupyter at http://localhost:8888 (token: `pipezone2024`)

```python
# Load your data
from connection_manager import ConnectionManager

cm = ConnectionManager()
config = cm.load_connection_config('postgres_production')

# Test connection
cm.test_connection('postgres_production')

# Use with Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.read \
    .format("jdbc") \
    .option("url", cm.get_connection_string('postgres_production')) \
    .option("dbtable", "public.customers") \
    .load()

df.show()
```

### 2. Test Flow Locally

```python
from flow_executor import FlowExecutor

executor = FlowExecutor()
result = executor.execute_flow('your_flow_name')
```

### 3. Deploy to Airflow

Simply enable the schedule in your flow YAML:

```yaml
schedule:
  enabled: true
  cron: "0 */6 * * *"
```

Airflow will automatically pick up the flow and create a DAG!

## üîê Security Best Practices

### Use Vault for Secrets

1. Store secrets in Vault:

```bash
docker exec -it pipezone_vault sh

vault kv put secret/postgres/production \
  username=myuser \
  password=mypassword
```

2. Reference in connection YAML:

```yaml
vault:
  enabled: true
  path: secret/data/postgres/production
  keys:
    username: username
    password: password
```

### Environment Variables

Never commit `.env` to git! Use `.env.example` as a template.

## üß™ Testing

### Test Connections

```python
from connection_manager import ConnectionManager

cm = ConnectionManager()

# List all connections
connections = cm.list_connections()

# Test each connection
for conn in connections:
    status = "‚úì" if cm.test_connection(conn) else "‚úó"
    print(f"{status} {conn}")
```

### Validate Flow Config

```python
from flow_executor import FlowExecutor

executor = FlowExecutor()
config = executor.load_flow_config('your_flow_name')
print(config)
```

## üì¶ Data Quality

Built-in data quality checks:

```yaml
data_quality:
  enabled: true
  checks:
    - name: row_count
      type: not_null
      threshold: 0

    - name: unique_id
      type: unique
      columns: [id]

    - name: email_format
      type: regex
      column: email
      pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

    - name: amount_range
      type: range
      column: amount
      min: 0
      max: 1000000
```

Results are stored in `data_quality_checks` table.

## üê≥ Docker Commands

```bash
# Start all services
bash setup/scripts/start-all.sh

# Stop all services
bash setup/scripts/stop-all.sh

# View logs
docker-compose -f setup/docker/docker-compose.infra.yml logs -f

# Restart a specific service
docker-compose -f setup/docker/docker-compose.airflow.yml restart airflow-scheduler

# Access container shell
docker exec -it pipezone_jupyter bash
docker exec -it pipezone_mysql mysql -u root -p
```

## üîß Troubleshooting

For detailed troubleshooting guide, see **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)**.

### Quick Fixes

**Services won't start:**
```bash
docker ps -a                    # Check container status
docker logs <container_name>    # Check logs
docker restart <container_name> # Restart service
```

**Airflow DAGs not appearing:**
```bash
docker logs pipezone_airflow_scheduler -f
docker exec pipezone_airflow_scheduler airflow dags reserialize
```

**Windows errors:**
- Use PowerShell or CMD scripts instead of bash
- Run as Administrator
- See [WINDOWS_SETUP.md](WINDOWS_SETUP.md) and [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

**Full reset (deletes all data):**
```bash
bash setup/scripts/stop-all.sh
docker-compose -f setup/docker/docker-compose.infra.yml down -v
docker network rm pipezone_network
bash setup/scripts/start-all.sh
```

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## üìÑ License

MIT License - see LICENSE file for details

## üôè Acknowledgments

Inspired by:
- Fivetran
- Airbyte
- Apache Airflow
- Delta Lake

---

**Built with ‚ù§Ô∏è for the data engineering community**

For questions and support, please open an issue on GitHub.
