{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üöÄ PipeZone - Getting Started\n",
    "\n",
    "This notebook demonstrates how to use PipeZone to extract, transform, and load data.\n",
    "\n",
    "## What you'll learn:\n",
    "1. How to work with connections\n",
    "2. How to execute flows\n",
    "3. How to access data in MinIO\n",
    "4. How to use Spark for transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/pipezone/utils')\n",
    "\n",
    "from connection_manager import ConnectionManager\n",
    "from flow_executor import FlowExecutor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import os\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connections",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Working with Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-connections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection manager\n",
    "cm = ConnectionManager()\n",
    "\n",
    "# List all available connections\n",
    "connections = cm.list_connections()\n",
    "print(\"üìã Available connections:\")\n",
    "for conn in connections:\n",
    "    print(f\"  ‚Ä¢ {conn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific connection configuration\n",
    "config = cm.load_connection_config('minio_raw')\n",
    "print(f\"\\nüì¶ Connection: {config['name']}\")\n",
    "print(f\"   Type: {config['type']}\")\n",
    "print(f\"   Description: {config['description']}\")\n",
    "print(f\"   Bucket: {config['connection']['bucket']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connections\n",
    "print(\"\\nüîç Testing connections:\")\n",
    "for conn in connections:\n",
    "    try:\n",
    "        status = \"‚úÖ\" if cm.test_connection(conn) else \"‚ùå\"\n",
    "        print(f\"  {status} {conn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  {conn} - {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flows",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Working with Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-flows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available flows\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "flows_path = '/opt/pipezone/metadata/flows'\n",
    "flows = [f.stem for f in Path(flows_path).glob('*.yml')]\n",
    "\n",
    "print(\"üìä Available flows:\")\n",
    "for flow in flows:\n",
    "    print(f\"  ‚Ä¢ {flow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-flow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a flow configuration\n",
    "executor = FlowExecutor()\n",
    "\n",
    "if flows:\n",
    "    flow_config = executor.load_flow_config(flows[0])\n",
    "    print(f\"\\nüìã Flow: {flow_config['name']}\")\n",
    "    print(f\"   Description: {flow_config['description']}\")\n",
    "    print(f\"   Source: {flow_config['source']['connection']}\")\n",
    "    print(f\"   Target: {flow_config['target']['connection']}\")\n",
    "    print(f\"   Schedule: {flow_config.get('schedule', {}).get('cron', 'Not scheduled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PipeZone Demo\") \\\n",
    "    .master(os.getenv('SPARK_MASTER', 'local[*]')) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample customer data\n",
    "sample_data = []\n",
    "for i in range(1000):\n",
    "    sample_data.append({\n",
    "        'customer_id': i + 1,\n",
    "        'name': f'Customer {i + 1}',\n",
    "        'email': f'customer{i + 1}@example.com',\n",
    "        'country': random.choice(['UZ', 'KZ', 'TJ', 'KG', 'TM']),\n",
    "        'created_at': datetime.now() - timedelta(days=random.randint(0, 365)),\n",
    "        'updated_at': datetime.now() - timedelta(hours=random.randint(0, 24))\n",
    "    })\n",
    "\n",
    "df = spark.createDataFrame(sample_data)\n",
    "print(f\"\\nüìä Created sample dataset with {df.count()} records\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data\n",
    "print(\"\\nüìà Data Analysis:\")\n",
    "df.groupBy('country').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "print(\"\\nüìä Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transform-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "df_transformed = df \\\n",
    "    .withColumn('email_domain', F.split(F.col('email'), '@').getItem(1)) \\\n",
    "    .withColumn('year', F.year('created_at')) \\\n",
    "    .withColumn('month', F.month('created_at')) \\\n",
    "    .withColumn('processed_at', F.current_timestamp())\n",
    "\n",
    "print(\"\\n‚ú® Transformed data:\")\n",
    "df_transformed.select('customer_id', 'email', 'email_domain', 'country', 'year', 'month').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minio",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Working with MinIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-to-minio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to MinIO (S3)\n",
    "minio_path = \"s3a://raw/demo/customers/\"\n",
    "\n",
    "df_transformed.write \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('country', 'year', 'month') \\\n",
    "    .option('compression', 'snappy') \\\n",
    "    .save(minio_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Data written to MinIO: {minio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read-from-minio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back from MinIO\n",
    "df_read = spark.read \\\n",
    "    .format('parquet') \\\n",
    "    .load(minio_path)\n",
    "\n",
    "print(f\"\\nüìñ Read {df_read.count()} records from MinIO\")\n",
    "df_read.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-minio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data from MinIO\n",
    "print(\"\\nüîç Query: Customers from Uzbekistan:\")\n",
    "df_read.filter(F.col('country') == 'UZ') \\\n",
    "    .select('customer_id', 'name', 'email', 'created_at') \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "1. **Create your own connections** in `metadata/connections/`\n",
    "2. **Define data flows** in `metadata/flows/`\n",
    "3. **Test flows** using `FlowExecutor`\n",
    "4. **Schedule flows** in Airflow by setting `schedule.enabled: true`\n",
    "5. **Monitor executions** in MySQL `pipeline_execution_logs` table\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- MinIO Console: http://localhost:9001\n",
    "- Airflow UI: http://localhost:8080\n",
    "- Spark UI: http://localhost:8081\n",
    "- Documentation: See README.md\n",
    "\n",
    "Happy data engineering! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
