# =============================================================================
# Notebook Runner DAG Template
# =============================================================================
# This is a template for auto-generated DAGs that run Jupyter notebooks
# Do not edit this file directly - it will be overwritten
# =============================================================================

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import papermill as pm
import os


# Default arguments for the DAG
default_args = {
    'owner': 'pipezone',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def run_notebook_with_papermill(
    notebook_path: str,
    output_path: str,
    parameters: dict,
    cluster_config: str = 'medium_cluster'
):
    """
    Execute a Jupyter notebook using Papermill.

    Args:
        notebook_path: Path to the input notebook
        output_path: Path for the output notebook
        parameters: Dictionary of parameters to inject
        cluster_config: Spark cluster configuration
    """

    # Set cluster environment variable
    os.environ['SELECTED_CLUSTER'] = cluster_config

    print(f"Executing notebook: {notebook_path}")
    print(f"Output: {output_path}")
    print(f"Parameters: {parameters}")
    print(f"Cluster: {cluster_config}")

    # Execute notebook
    pm.execute_notebook(
        input_path=notebook_path,
        output_path=output_path,
        parameters=parameters,
        kernel_name='python3',
        progress_bar=True,
        report_mode=True
    )

    print(f"✓ Notebook execution completed: {output_path}")

    # Upload to MinIO (optional)
    try:
        from minio import Minio

        minio_endpoint = os.getenv('MINIO_ENDPOINT', 'minio:9000').replace('http://', '')
        minio_client = Minio(
            minio_endpoint,
            access_key=os.getenv('MINIO_ACCESS_KEY', 'admin'),
            secret_key=os.getenv('MINIO_SECRET_KEY', 'changeme123'),
            secure=False
        )

        bucket_name = 'notebook-outputs'
        object_name = os.path.basename(output_path)

        if not minio_client.bucket_exists(bucket_name):
            minio_client.make_bucket(bucket_name)

        minio_client.fput_object(bucket_name, object_name, output_path)
        print(f"✓ Uploaded to MinIO: s3://{bucket_name}/{object_name}")

    except Exception as e:
        print(f"Warning: Failed to upload to MinIO: {e}")


# Example DAG (will be customized by generator)
with DAG(
    dag_id='notebook_example',
    default_args=default_args,
    description='Example notebook execution DAG',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['notebook', 'pipezone'],
) as dag:

    run_notebook = PythonOperator(
        task_id='run_notebook',
        python_callable=run_notebook_with_papermill,
        op_kwargs={
            'notebook_path': '/shared/notebooks/examples/example.ipynb',
            'output_path': '/tmp/output_{{ ds_nodash }}.ipynb',
            'parameters': {
                'execution_date': '{{ ds }}',
                'run_id': '{{ run_id }}'
            },
            'cluster_config': 'medium_cluster'
        }
    )
