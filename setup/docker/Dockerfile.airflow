# ==============================================
# PIPEZONE - Custom Airflow with PySpark + Drivers
# ==============================================

FROM apache/airflow:2.8.1-python3.11

USER root

# Install system dependencies for database drivers
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    build-essential \
    default-libmysqlclient-dev \
    libpq-dev \
    unixodbc-dev \
    freetds-dev \
    libsasl2-dev \
    libssl-dev \
    openjdk-17-jdk-headless \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME for Spark
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

USER airflow

# Install PySpark and database drivers
RUN pip install --no-cache-dir \
    # PySpark
    pyspark==3.4.2 \
    \
    # Database drivers - PostgreSQL
    psycopg2-binary==2.9.9 \
    \
    # Database drivers - MySQL/MariaDB
    mysql-connector-python==8.2.0 \
    pymysql==1.1.0 \
    mysqlclient==2.2.1 \
    \
    # Database drivers - SQL Server
    pyodbc==5.0.1 \
    pymssql==2.2.11 \
    \
    # Database drivers - Oracle
    cx-Oracle==8.3.0 \
    oracledb==2.0.1 \
    \
    # Database drivers - MongoDB
    pymongo==4.6.1 \
    motor==3.3.2 \
    \
    # Database drivers - Cassandra
    cassandra-driver==3.29.0 \
    \
    # Database drivers - Redis
    redis==5.0.1 \
    hiredis==2.3.2 \
    \
    # Database drivers - Elasticsearch
    elasticsearch==8.11.1 \
    \
    # Database drivers - ClickHouse
    clickhouse-driver==0.2.7 \
    clickhouse-connect==0.7.0 \
    \
    # Database drivers - Snowflake
    snowflake-connector-python==3.6.0 \
    snowflake-sqlalchemy==1.5.1 \
    \
    # Object Storage - MinIO/S3
    boto3==1.34.24 \
    minio==7.2.3 \
    s3fs==2024.2.0 \
    \
    # Secrets Management - Vault
    hvac==2.1.0 \
    \
    # SQLAlchemy
    sqlalchemy==2.0.25 \
    alembic==1.13.1 \
    sqlparse==0.4.4 \
    \
    # Data processing
    pandas==2.1.4 \
    numpy==1.26.3 \
    pyarrow==14.0.2 \
    fastparquet==2024.2.0 \
    \
    # Data validation
    great-expectations==0.18.8 \
    pandera==0.18.0 \
    \
    # Utilities
    python-dotenv==1.0.1 \
    pyyaml==6.0.1 \
    requests==2.31.0 \
    tenacity==8.2.3 \
    tqdm==4.66.1

# Download JDBC drivers for Spark
USER root

RUN mkdir -p /opt/airflow/jars && \
    cd /opt/airflow/jars && \
    # PostgreSQL JDBC driver
    wget -q https://jdbc.postgresql.org/download/postgresql-42.7.1.jar && \
    # MySQL JDBC driver
    wget -q https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.2.0/mysql-connector-j-8.2.0.jar && \
    # SQL Server JDBC driver
    wget -q https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.4.2.jre11/mssql-jdbc-12.4.2.jre11.jar && \
    # Oracle JDBC driver
    wget -q https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/23.3.0.23.09/ojdbc8-23.3.0.23.09.jar && \
    # ClickHouse JDBC driver
    wget -q https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.5.0/clickhouse-jdbc-0.5.0-all.jar && \
    # AWS SDK for Spark (S3/MinIO support)
    wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
    chown -R airflow:root /opt/airflow/jars

USER airflow

# Set Spark environment variables
ENV SPARK_HOME=/home/airflow/.local/lib/python3.11/site-packages/pyspark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

# Spark configuration for MinIO/S3
ENV SPARK_OPTS="--jars /opt/airflow/jars/hadoop-aws-3.3.4.jar,/opt/airflow/jars/aws-java-sdk-bundle-1.12.262.jar"
