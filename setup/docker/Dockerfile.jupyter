# ==============================================
# PIPEZONE - Jupyter Notebook with PySpark + Database Drivers
# ==============================================

ARG SPARK_VERSION=3.5.0
FROM jupyter/pyspark-notebook:latest

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    vim \
    git \
    build-essential \
    unixodbc \
    unixodbc-dev \
    freetds-dev \
    freetds-bin \
    tdsodbc \
    libkrb5-dev \
    libsasl2-dev \
    libssl-dev \
    libpq-dev \
    default-libmysqlclient-dev \
    && rm -rf /var/lib/apt/lists/*

# Switch to jovyan user for pip installations
USER ${NB_UID}

# Install Python packages
RUN pip install --no-cache-dir \
    # Data processing
    pandas==2.1.4 \
    numpy==1.26.3 \
    pyarrow==14.0.2 \
    fastparquet==2024.2.0 \
    \
    # Database drivers - PostgreSQL
    psycopg2-binary==2.9.9 \
    \
    # Database drivers - MySQL/MariaDB
    mysql-connector-python==8.2.0 \
    pymysql==1.1.0 \
    mysqlclient==2.2.1 \
    \
    # Database drivers - SQL Server
    pyodbc==5.0.1 \
    pymssql==2.2.11 \
    \
    # Database drivers - Oracle
    cx-Oracle==8.3.0 \
    oracledb==2.0.1 \
    \
    # Database drivers - MongoDB
    pymongo==4.6.1 \
    motor==3.3.2 \
    \
    # Database drivers - Cassandra
    cassandra-driver==3.29.0 \
    \
    # Database drivers - Redis
    redis==5.0.1 \
    hiredis==2.3.2 \
    \
    # Database drivers - Elasticsearch
    elasticsearch==8.11.1 \
    \
    # Database drivers - ClickHouse
    clickhouse-driver==0.2.7 \
    clickhouse-connect==0.7.0 \
    \
    # Database drivers - Snowflake
    snowflake-connector-python==3.6.0 \
    snowflake-sqlalchemy==1.5.1 \
    \
    # Object Storage
    boto3==1.34.24 \
    minio==7.2.3 \
    s3fs==2024.2.0 \
    \
    # Secrets Management
    hvac==2.1.0 \
    \
    # SQLAlchemy & query tools
    sqlalchemy==2.0.25 \
    alembic==1.13.1 \
    sqlparse==0.4.4 \
    \
    # Data validation
    great-expectations==0.18.8 \
    pandera==0.18.0 \
    \
    # Workflow & orchestration clients
    apache-airflow-client==2.8.0 \
    \
    # Utilities
    python-dotenv==1.0.1 \
    pyyaml==6.0.1 \
    requests==2.31.0 \
    tenacity==8.2.3 \
    tqdm==4.66.1 \
    \
    # Notebooks
    jupyterlab==4.0.11 \
    jupyterlab-git==0.50.0 \
    ipywidgets==8.1.1 \
    \
    # Visualization
    matplotlib==3.8.2 \
    seaborn==0.13.1 \
    plotly==5.18.0

# Download JDBC drivers for Spark
USER root

RUN mkdir -p /opt/spark/jars && \
    # PostgreSQL JDBC driver
    wget -q https://jdbc.postgresql.org/download/postgresql-42.7.1.jar -O /opt/spark/jars/postgresql-42.7.1.jar && \
    # MySQL JDBC driver
    wget -q https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.2.0/mysql-connector-j-8.2.0.jar -O /opt/spark/jars/mysql-connector-j-8.2.0.jar && \
    # SQL Server JDBC driver
    wget -q https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.4.2.jre11/mssql-jdbc-12.4.2.jre11.jar -O /opt/spark/jars/mssql-jdbc-12.4.2.jre11.jar && \
    # Oracle JDBC driver (ojdbc8)
    wget -q https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/23.3.0.23.09/ojdbc8-23.3.0.23.09.jar -O /opt/spark/jars/ojdbc8-23.3.0.23.09.jar && \
    # ClickHouse JDBC driver
    wget -q https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.5.0/clickhouse-jdbc-0.5.0-all.jar -O /opt/spark/jars/clickhouse-jdbc-0.5.0-all.jar && \
    # AWS SDK for Spark (S3 support)
    wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -O /opt/spark/jars/hadoop-aws-3.3.4.jar && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar -O /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar

# Set proper permissions
RUN chown -R ${NB_UID}:${NB_GID} /opt/spark/jars

# Create workspace directories
RUN mkdir -p /home/${NB_USER}/notebooks && \
    mkdir -p /home/${NB_USER}/jobs && \
    mkdir -p /home/${NB_USER}/transformations && \
    mkdir -p /home/${NB_USER}/utils && \
    mkdir -p /home/${NB_USER}/drivers && \
    chown -R ${NB_UID}:${NB_GID} /home/${NB_USER}

USER ${NB_UID}

# Set Spark environment variables
ENV SPARK_OPTS="--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info"
ENV PYSPARK_SUBMIT_ARGS="--master spark://spark-master:7077 pyspark-shell"

WORKDIR /home/${NB_USER}

EXPOSE 8888
