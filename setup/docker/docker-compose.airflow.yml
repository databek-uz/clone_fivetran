# ==============================================
# PIPEZONE - Airflow Orchestration
# ==============================================

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_CORE_LOAD_EXAMPLES}
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    AIRFLOW__CORE__DAGS_FOLDER: ${AIRFLOW_CORE_DAGS_FOLDER}
    AIRFLOW__CORE__PLUGINS_FOLDER: ${AIRFLOW_CORE_PLUGINS_FOLDER}

    # Vault integration
    VAULT_ADDR: ${VAULT_ADDR}
    VAULT_TOKEN: ${VAULT_TOKEN}

    # MinIO integration
    MINIO_ROOT_USER: ${MINIO_ROOT_USER}
    MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    MINIO_HOST: ${MINIO_HOST}
    MINIO_PORT: ${MINIO_PORT}

    # MySQL metadata
    MYSQL_HOST: ${MYSQL_HOST}
    MYSQL_PORT: ${MYSQL_PORT}
    MYSQL_DATABASE: ${MYSQL_DATABASE}
    MYSQL_USER: ${MYSQL_USER}
    MYSQL_PASSWORD: ${MYSQL_PASSWORD}

    # Spark integration
    SPARK_MASTER: ${SPARK_MASTER}

    # Metadata paths
    METADATA_PATH: ${METADATA_PATH}
    CONNECTIONS_PATH: ${CONNECTIONS_PATH}
    FLOWS_PATH: ${FLOWS_PATH}
    SCHEMAS_PATH: ${SCHEMAS_PATH}

    # Timezone
    TZ: ${TZ}

  volumes:
    - ../../schedule/airflow/dags:/opt/airflow/dags
    - ../../schedule/airflow/logs:/opt/airflow/logs
    - ../../schedule/airflow/plugins:/opt/airflow/plugins
    - ../../schedule/airflow/config:/opt/airflow/config
    - ../../metadata:/opt/pipezone/metadata:ro
    - ../../core:/opt/pipezone/core:ro
  user: "${AIRFLOW_UID:-50000}:0"
  networks:
    - pipezone_network

services:
  # -----------------
  # Airflow Database Init
  # -----------------
  airflow-init:
    <<: *airflow-common
    container_name: pipezone_airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Create Airflow database if not exists
        mysql -h mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "CREATE DATABASE IF NOT EXISTS airflow CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;"
        mysql -h mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "CREATE USER IF NOT EXISTS '${AIRFLOW_DATABASE_USER}'@'%' IDENTIFIED BY '${AIRFLOW_DATABASE_PASSWORD}';"
        mysql -h mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "GRANT ALL PRIVILEGES ON airflow.* TO '${AIRFLOW_DATABASE_USER}'@'%';"
        mysql -h mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "FLUSH PRIVILEGES;"

        # Initialize Airflow database
        airflow db init

        # Create admin user
        airflow users create \
          --username ${AIRFLOW_ADMIN_USERNAME} \
          --password ${AIRFLOW_ADMIN_PASSWORD} \
          --firstname ${AIRFLOW_ADMIN_FIRSTNAME} \
          --lastname ${AIRFLOW_ADMIN_LASTNAME} \
          --role Admin \
          --email ${AIRFLOW_ADMIN_EMAIL} || true

        echo "Airflow initialized successfully"
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
    restart: "no"

  # -----------------
  # Airflow Webserver
  # -----------------
  airflow-webserver:
    <<: *airflow-common
    container_name: pipezone_airflow_webserver
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # -----------------
  # Airflow Scheduler
  # -----------------
  airflow-scheduler:
    <<: *airflow-common
    container_name: pipezone_airflow_scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # -----------------
  # Airflow Triggerer
  # -----------------
  airflow-triggerer:
    <<: *airflow-common
    container_name: pipezone_airflow_triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

networks:
  pipezone_network:
    external: true
    name: ${NETWORK_NAME}
