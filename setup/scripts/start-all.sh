#!/bin/bash

# ==============================================
# PipeZone - Start All Services
# ==============================================

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"
DOCKER_DIR="${PROJECT_ROOT}/setup/docker"

echo "üöÄ Starting PipeZone Platform..."
echo "================================"

# Load environment variables
if [ -f "${PROJECT_ROOT}/.env" ]; then
    echo "‚úì Loading environment variables from .env"
    export $(cat "${PROJECT_ROOT}/.env" | grep -v '^#' | xargs)
else
    echo "‚ùå Error: .env file not found!"
    exit 1
fi

cd "${DOCKER_DIR}"

# Step 1: Start Infrastructure Services
echo ""
echo "üì¶ Step 1: Starting Infrastructure Services (MySQL, MinIO, Vault)..."
docker-compose -f docker-compose.infra.yml up -d

echo "‚è≥ Waiting for infrastructure to be ready..."
sleep 15

# Check MySQL health
echo "   Checking MySQL..."
until docker exec pipezone_mysql mysqladmin ping -h localhost -u root -p${MYSQL_ROOT_PASSWORD} --silent; do
    echo "   Waiting for MySQL..."
    sleep 2
done
echo "   ‚úì MySQL is ready"

# Check MinIO health
echo "   Checking MinIO..."
until curl -sf http://localhost:${MINIO_PORT}/minio/health/live > /dev/null 2>&1; do
    echo "   Waiting for MinIO..."
    sleep 2
done
echo "   ‚úì MinIO is ready"

# Step 2: Start Airflow
echo ""
echo "‚úàÔ∏è  Step 2: Starting Airflow Services..."
docker-compose -f docker-compose.airflow.yml up -d

echo "‚è≥ Waiting for Airflow to initialize..."
sleep 20

# Step 3: Start Jupyter + Spark
echo ""
echo "üìì Step 3: Starting Jupyter Notebook + Spark..."
docker-compose -f docker-compose.notebooks.yml up -d

echo "‚è≥ Waiting for services to start..."
sleep 10

echo ""
echo "================================"
echo "‚úÖ PipeZone Platform Started Successfully!"
echo ""
echo "üìä Access URLs:"
echo "   ‚Ä¢ Airflow UI:        http://localhost:${AIRFLOW_WEBSERVER_PORT}"
echo "   ‚Ä¢ MinIO Console:     http://localhost:${MINIO_CONSOLE_PORT}"
echo "   ‚Ä¢ Vault UI:          http://localhost:8200"
echo "   ‚Ä¢ Jupyter Notebook:  http://localhost:${JUPYTER_PORT}"
echo "   ‚Ä¢ Spark Master UI:   http://localhost:${SPARK_MASTER_WEBUI_PORT}"
echo "   ‚Ä¢ Spark Worker UI:   http://localhost:${SPARK_WORKER_WEBUI_PORT}"
echo ""
echo "üîê Credentials:"
echo "   ‚Ä¢ Airflow:   username=${AIRFLOW_ADMIN_USERNAME}, password=${AIRFLOW_ADMIN_PASSWORD}"
echo "   ‚Ä¢ MinIO:     username=${MINIO_ROOT_USER}, password=${MINIO_ROOT_PASSWORD}"
echo "   ‚Ä¢ Jupyter:   token=${JUPYTER_TOKEN}"
echo "   ‚Ä¢ Vault:     token=${VAULT_TOKEN}"
echo ""
echo "üìù Check logs:"
echo "   docker-compose -f setup/docker/docker-compose.infra.yml logs -f"
echo "   docker-compose -f setup/docker/docker-compose.airflow.yml logs -f"
echo "   docker-compose -f setup/docker/docker-compose.notebooks.yml logs -f"
echo ""
